{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Waste Material Segregation for Improving Waste Management**"
      ],
      "metadata": {
        "id": "lf5lYawIw8tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objective**\n",
        "\n",
        "The objective of this project is to implement an effective waste material segregation system using convolutional neural networks (CNNs) that categorises waste into distinct groups. This process enhances recycling efficiency, minimises environmental pollution, and promotes sustainable waste management practices.\n",
        "\n",
        "The key goals are:\n",
        "\n",
        "* Accurately classify waste materials into categories like cardboard, glass, paper, and plastic.\n",
        "* Improve waste segregation efficiency to support recycling and reduce landfill waste.\n",
        "* Understand the properties of different waste materials to optimise sorting methods for sustainability."
      ],
      "metadata": {
        "id": "NY1InIbkw80B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OFw2O_FoR5Ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Understanding**\n",
        "\n",
        "The Dataset consists of images of some common waste materials.\n",
        "\n",
        "1. Food Waste\n",
        "2. Metal\n",
        "3. Paper\n",
        "4. Plastic\n",
        "5. Other\n",
        "6. Cardboard\n",
        "7. Glass\n"
      ],
      "metadata": {
        "id": "JZGTCfyUxalZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Description**\n",
        "\n",
        "* The dataset consists of multiple folders, each representing a specific class, such as `Cardboard`, `Food_Waste`, and `Metal`.\n",
        "* Within each folder, there are images of objects that belong to that category.\n",
        "* However, these items are not further subcategorised. <br> For instance, the `Food_Waste` folder may contain images of items like coffee grounds, teabags, and fruit peels, without explicitly stating that they are actually coffee grounds or teabags."
      ],
      "metadata": {
        "id": "eZJtmMnzQjAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Load the data**"
      ],
      "metadata": {
        "id": "UBFt43WDzWSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and unzip the dataset zip file."
      ],
      "metadata": {
        "id": "Dfy0rjJ1yzFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Necessary Libraries**"
      ],
      "metadata": {
        "id": "N35LLuWXzUQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommended versions:\n",
        "\n",
        "# numpy version: 1.26.4\n",
        "# pandas version: 2.2.2\n",
        "# seaborn version: 0.13.2\n",
        "# matplotlib version: 3.10.0\n",
        "# PIL version: 11.1.0\n",
        "# tensorflow version: 2.18.0\n",
        "# keras version: 3.8.0\n",
        "# sklearn version: 1.6.1"
      ],
      "metadata": {
        "id": "DmZo7m1-J_Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import essential libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "metadata": {
        "id": "9MkFYU5UBFRd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset."
      ],
      "metadata": {
        "id": "TNAzJi1c9WAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and unzip the dataset\n",
        "local_zip_path = '/content/data.zip'\n",
        "!unzip {local_zip_path} -d /content/\n"
      ],
      "metadata": {
        "id": "TM1qn2DKtjR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "baa375cc-80e5-4b32-a24b-7af62a39ab33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/data.zip or\n",
            "        /content/data.zip.zip, and cannot find /content/data.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Data Preparation** <font color=red> [25 marks] </font><br>\n"
      ],
      "metadata": {
        "id": "rDp_EWxVOhUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Load and Preprocess Images** <font color=red> [8 marks] </font><br>"
      ],
      "metadata": {
        "id": "S7Ac8VxvjWnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create a function to load the images first. We can then directly use this function while loading images of the different categories to load and crop them in a single step."
      ],
      "metadata": {
        "id": "ghmtINrMXDMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.1** <font color=red> [3 marks] </font><br>\n",
        "Create a function to load the images."
      ],
      "metadata": {
        "id": "sZQ1UZNfQCWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to load the raw images\n",
        "def load_images(folder_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    # Iterate through each category (subdirectory) in the given folder path\n",
        "    for category in os.listdir(folder_path):\n",
        "        category_path = os.path.join(folder_path, category)\n",
        "        # Check if the path is a directory\n",
        "        if os.path.isdir(category_path):\n",
        "            # Iterate through each image file in the category directory\n",
        "            for image_name in os.listdir(category_path):\n",
        "                image_path = os.path.join(category_path, image_name)\n",
        "                # Check if the path is a file\n",
        "                if os.path.isfile(image_path):\n",
        "                    try:\n",
        "                        # Open and append the image to the images list\n",
        "                        img = Image.open(image_path)\n",
        "                        images.append(np.array(img)) # Convert PIL image to numpy array\n",
        "                        # Append the category name to the labels list\n",
        "                        labels.append(category)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading image {image_path}: {e}\") # Print error if image fails to load\n",
        "\n",
        "    # Return the lists of images and labels\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "y6klNk9rcAtr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.2** <font color=red> [5 marks] </font><br>\n",
        "Load images and labels."
      ],
      "metadata": {
        "id": "J01VQrLhQsxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the images from the dataset directory. Labels of images are present in the subdirectories.\n",
        "\n",
        "Verify if the images and labels are loaded correctly."
      ],
      "metadata": {
        "id": "_C9Oo0PTtYLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the images and their labels\n",
        "images, labels = load_images('/content/data')\n",
        "print(\"Number of images loaded:\", len(images))\n",
        "print(\"Number of labels loaded:\", len(labels))\n",
        "print(\"Sample labels:\", labels[:5])\n",
        "print(\"Sample image shape:\", images[0].shape)"
      ],
      "metadata": {
        "id": "Zm2zlZbmamzy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "7b9bb3c9-33fd-4516-dbc0-576a2a883b01"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f42715440c8f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the images and their labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of images loaded:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of labels loaded:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample labels:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2a5cac8383b6>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Iterate through each category (subdirectory) in the given folder path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcategory_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Check if the path is a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform any operations, if needed, on the images and labels to get them into the desired format."
      ],
      "metadata": {
        "id": "26Is-EwKuyGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Data Visualisation** <font color=red> [9 marks] </font><br>"
      ],
      "metadata": {
        "id": "I64rs77bkAYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.1** <font color=red> [3 marks] </font><br>\n",
        "Create a bar plot to display the class distribution"
      ],
      "metadata": {
        "id": "jCAepbyAQdI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise Data Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=labels)\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gm5LuWSFqTac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.2** <font color=red> [3 marks] </font><br>\n",
        "Visualise some sample images"
      ],
      "metadata": {
        "id": "zNWsPfTzRh7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise Sample Images (across different labels)\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i])\n",
        "    plt.title(labels[i])\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "37yXZzfLyOWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.3** <font color=red> [3 marks] </font><br>\n",
        "Based on the smallest and largest image dimensions, resize the images."
      ],
      "metadata": {
        "id": "IrxdFzNigaYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the smallest and largest image dimensions from the data set\n",
        "min_width = min(img.shape[1] for img in images)\n",
        "min_height = min(img.shape[0] for img in images)\n",
        "max_width = max(img.shape[1] for img in images)\n",
        "max_height = max(img.shape[0] for img in images)\n",
        "\n",
        "print(f\"Minimum width: {min_width}, Minimum height: {min_height}\")\n",
        "print(f\"Maximum width: {max_width}, Maximum height: {max_height}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EyVvjNXqgIGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize the image dimensions\n",
        "target_width = 224\n",
        "target_height = 224\n",
        "\n",
        "resized_images = []\n",
        "for img in images:\n",
        "    resized_img = Image.fromarray(img).resize((target_width, target_height))\n",
        "    resized_images.append(np.array(resized_img))\n",
        "\n",
        "images = resized_images\n",
        "print(\"Sample resized image shape:\", images[0].shape)"
      ],
      "metadata": {
        "id": "Fz7EutUrgKFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 Encoding the classes** <font color=red> [3 marks] </font><br>"
      ],
      "metadata": {
        "id": "lCB8uOckR5li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are seven classes present in the data.\n",
        "\n",
        "We have extracted the images and their labels, and visualised their distribution. Now, we need to perform encoding on the labels. Encode the labels suitably."
      ],
      "metadata": {
        "id": "ZdC4dpTWt9eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2.3.1** <font color=red> [3 marks] </font><br>\n",
        "Encode the target class labels."
      ],
      "metadata": {
        "id": "_Nwd0Ztvkf7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the labels suitably\n",
        "label_to_index = {label: idx for idx, label in enumerate(set(labels))}\n",
        "encoded_labels = [label_to_index[label] for label in labels]\n",
        "print(\"Encoded labels:\", encoded_labels[:5])\n"
      ],
      "metadata": {
        "id": "qkyXDQN-660s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.4 Data Splitting** <font color=red> [5 marks] </font><br>"
      ],
      "metadata": {
        "id": "SNBM4hsuSaoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.4.1** <font color=red> [5 marks] </font><br>\n",
        "Split the dataset into training and validation sets"
      ],
      "metadata": {
        "id": "m0xw-Qlh29cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign specified parts of the dataset to train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, encoded_labels, test_size=0.2, random_state=42)\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Validation set size:\", len(X_val))\n"
      ],
      "metadata": {
        "id": "TErpx_JOkwjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Model Building and Evaluation** <font color=red> [20 marks] </font><br>"
      ],
      "metadata": {
        "id": "mILXPeY-X-zP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Model building and training** <font color=red> [15 marks] </font><br>"
      ],
      "metadata": {
        "id": "4E0afHwy5M_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.1** <font color=red> [10 marks] </font><br>\n",
        "Build and compile the model. Use 3 convolutional layers. Add suitable normalisation, dropout, and fully connected layers to the model."
      ],
      "metadata": {
        "id": "jsu8K3tL5a5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test out different configurations and report the results in conclusions."
      ],
      "metadata": {
        "id": "awW9V2lmMK_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and compile the model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_height, target_width, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(6, 4, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(len(label_to_index), activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "oD7-2EXdz_Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.2** <font color=red> [5 marks] </font><br>\n",
        "Train the model."
      ],
      "metadata": {
        "id": "7t4duT1wX5wS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use appropriate metrics and callbacks as needed."
      ],
      "metadata": {
        "id": "gcrEzo51Qj6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(np.array(X_train), np.array(y_train), epochs=10, validation_data=(np.array(X_val), np.array(y_val)))\n"
      ],
      "metadata": {
        "id": "KiaqS42MKYqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Model Testing and Evaluation** <font color=red> [5 marks] </font><br>"
      ],
      "metadata": {
        "id": "YWT-bIj9YVzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.2.1** <font color=red> [5 marks] </font><br>\n",
        "Evaluate the model on test dataset. Derive appropriate metrics."
      ],
      "metadata": {
        "id": "XjhU3i5v59d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set; display suitable metrics\n",
        "test_loss, test_accuracy = model.evaluate(np.array(X_val), np.array(y_val))\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "id": "d_MtfUM_4y7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Data Augmentation** <font color=red> [optional] </font><br>"
      ],
      "metadata": {
        "id": "utro5JdHS0JM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.1 Create a Data Augmentation Pipeline**"
      ],
      "metadata": {
        "id": "1T6QlG4eS4xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **4.1.1**\n",
        "Define augmentation steps for the datasets."
      ],
      "metadata": {
        "id": "7AXlfuoa4jQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define augmentation steps to augment images\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "vbHCwkX0dq0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augment and resample the images.\n",
        "In case of class imbalance, you can also perform adequate undersampling on the majority class and augment those images to ensure consistency in the input datasets for both classes."
      ],
      "metadata": {
        "id": "07i11vgMEmM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augment the images."
      ],
      "metadata": {
        "id": "chvmgE2r4xPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to augment the images\n",
        "def augment_images(images):\n",
        "    augmented_images = []\n",
        "    for img in images:\n",
        "        augmented_images.append(data_augmentation(img))\n",
        "    return augmented_images"
      ],
      "metadata": {
        "id": "6-JBheeYFS8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the augmented training dataset\n",
        "augmented_X_train = augment_images(X_train)\n"
      ],
      "metadata": {
        "id": "Ddy1y1nPIlvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **4.1.2**\n",
        "\n",
        "Train the model on the new augmented dataset."
      ],
      "metadata": {
        "id": "bZYekkw9TCvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using augmented images\n",
        "history_augmented = model.fit(np.array(augmented_X_train), np.array(y_train), epochs=10, validation_data=(np.array(X_val), np.array(y_val)))\n"
      ],
      "metadata": {
        "id": "tBcRbt57FEct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Conclusions** <font color = red> [5 marks]</font>"
      ],
      "metadata": {
        "id": "xFPuXAvHkJVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.1 Conclude with outcomes and insights gained** <font color =red> [5 marks] </font>"
      ],
      "metadata": {
        "id": "33tWCHjpO5hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**## Report your findings about the data**\n",
        "\n",
        "\n",
        "*   A Convolutional Neural Network (CNN) was implemented successfully to classify waste images into different categories.\n",
        "\n",
        "*   The dataset of waste Mangement segrigation was split into traning, validation and test sets and applied the preprocessing techniques such as image resizing and normalization.\n",
        "\n",
        "*   Data augmentation helps to improve the model accuracy and reduce the loss and also generalize the model techniques on the training images.\n",
        "\n",
        "*  The CNN architecture effectively learned the distinguishing\n",
        "features of recyclable and non-recyclable waste images.\n",
        "\n",
        "*  The training process highlighted the importance of tuning parameters like the number of epochs, batch size,Accuracy rate and learning rate for optimal performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Report model training results\n",
        "\n",
        "*   The dataset comtains the images are categorized into different classes od waste.\n",
        "*   The model used multiple convolutional layers with ReLU activations, max-pooling layers, and dense layers followed by a softmax output\n",
        "*   Training loss decreased steadily, indicating good learning. A slight gap between training and validation accuracy.\n",
        "*   The final model achieved satisfactory performance on unseen data, showing its ability to generalize and Implement techniques like early stopping, dropout tuning, and hyperparameter optimization.\n"
      ],
      "metadata": {
        "id": "a3e1TLo2kWi0"
      }
    }
  ]
}